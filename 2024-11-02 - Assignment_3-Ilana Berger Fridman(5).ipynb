{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96816ed7-b08a-4ca3-abb9-f99880c3535d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Overview\n",
    "\n",
    "This notebook will show you how to create and query a table or DataFrame that you uploaded to DBFS. [DBFS](https://docs.databricks.com/user-guide/dbfs-databricks-file-system.html) is a Databricks File System that allows you to store data for querying inside of Databricks. This notebook assumes that you have a file already inside of DBFS that you would like to read from.\n",
    "\n",
    "This notebook is written in **Python** so the default cell type is Python. However, you can use different languages by using the `%LANGUAGE` syntax. Python, Scala, SQL, and R are all supported."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e2b5e00-9bc6-4ab3-af73-de2899ce7e31",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "***Assignment 3***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9078fff1-ab01-4648-9e86-35a53e0adfd7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "*Uploading the data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "git add your_file.sql\n",
    "git commit -m \"Add SQL file\"\n",
    "git push"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6482be4c-f067-47c9-b0ac-35c938b94601",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------+----+----+-------------+---------+--------------------+--------+---------------+--------+------+---------+-------+----+-------------+\n",
      "|battery_level|c02_level|cca2|cca3|           cn|device_id|         device_name|humidity|             ip|latitude|   lcd|longitude|  scale|temp|    timestamp|\n",
      "+-------------+---------+----+----+-------------+---------+--------------------+--------+---------------+--------+------+---------+-------+----+-------------+\n",
      "|            8|      868|  US| USA|United States|        1|meter-gauge-1xbYRYcj|      51|   68.161.225.1|    38.0| green|    -97.0|Celsius|  34|1458444054093|\n",
      "|            7|     1473|  NO| NOR|       Norway|        2|   sensor-pad-2n2Pea|      70|  213.161.254.1|   62.47|   red|     6.15|Celsius|  11|1458444054119|\n",
      "|            2|     1556|  IT| ITA|        Italy|        3| device-mac-36TWSKiT|      44|      88.36.5.1|   42.83|   red|    12.83|Celsius|  19|1458444054120|\n",
      "|            6|     1080|  US| USA|United States|        4|   sensor-pad-4mzWkz|      32|  66.39.173.154|   44.06|yellow|  -121.32|Celsius|  28|1458444054121|\n",
      "|            4|      931|  PH| PHL|  Philippines|        5|therm-stick-5gimp...|      62|    203.82.41.9|   14.58| green|   120.97|Celsius|  25|1458444054122|\n",
      "|            3|     1210|  US| USA|United States|        6|sensor-pad-6al7RT...|      51| 204.116.105.67|   35.93|yellow|   -85.46|Celsius|  27|1458444054122|\n",
      "|            3|     1129|  CN| CHN|        China|        7|meter-gauge-7GeDoanM|      26|  220.173.179.1|   22.82|yellow|   108.32|Celsius|  18|1458444054123|\n",
      "|            0|     1536|  JP| JPN|        Japan|        8|sensor-pad-8xUD6p...|      35|  210.173.177.1|   35.69|   red|   139.69|Celsius|  27|1458444054123|\n",
      "|            3|      807|  JP| JPN|        Japan|        9| device-mac-9GcjZ2pw|      85|  118.23.68.227|   35.69| green|   139.69|Celsius|  13|1458444054124|\n",
      "|            7|     1470|  US| USA|United States|       10|sensor-pad-10Bsyw...|      56|208.109.163.218|   33.61|   red|  -111.89|Celsius|  26|1458444054125|\n",
      "+-------------+---------+----+----+-------------+---------+--------------------+--------+---------------+--------+------+---------+-------+----+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# File location and type\n",
    "file_location = \"/FileStore/tables/iot_devices.json\"\n",
    "file_type = \"json\"\n",
    "\n",
    "# CSV options\n",
    "infer_schema = \"false\"\n",
    "first_row_is_header = \"false\"\n",
    "delimiter = \",\"\n",
    "\n",
    "# The applied options are for CSV files. For other file types, these will be ignored.\n",
    "df = spark.read.format(file_type) \\\n",
    "  .option(\"inferSchema\", infer_schema) \\\n",
    "  .option(\"header\", first_row_is_header) \\\n",
    "  .option(\"sep\", delimiter) \\\n",
    "  .load(file_location)\n",
    "\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4efd613-eef7-4401-beee-5242643f8a71",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "***Question (1)***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b9fb1dc-2fe2-48ff-a3d5-299e5a6fd5fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "***Definitions:*** \n",
    "\n",
    "**Resilient Distributed Datasets (RDDs):** RDDs are the foundational distributed data structure in Spark, representing immutable, fault-tolerant collections that can be processed in parallel.\n",
    "\n",
    "**Dataframes:** DataFrames build on RDDs by adding a schema to each collection, making the data accessible in a structured, tabular format similar to a database table.\n",
    "\n",
    "**Datasets:** Datasets are a hybrid between RDDs and DataFrames, offering both schema enforcement and compile-time type safety.\n",
    "\n",
    "\n",
    "**RDDs:**\n",
    "\n",
    "*Key Features:*\n",
    "\n",
    "-\tControl and Flexibility: RDDs provide full control over data distribution and low-level operations.\n",
    "\n",
    "-\tLazy Evaluation: Transformations on RDDs are not executed until an action (like count() or collect()) is invoked, which helps optimize execution plans.\n",
    "\n",
    "-\tNo Schema: They do not have a predefined schema, making them versatile but more challenging to optimize.\n",
    "\n",
    "*Performance Considerations:*\n",
    "\n",
    "Due to their lack of schema and optimizations, RDDs are less efficient than DataFrames or Datasets.\n",
    "\n",
    "**Ideal Use Case:** Suitable for unstructured data or complex transformations where fine-grained control over data is necessary.\n",
    "\n",
    "\n",
    "**DataFrames:**\n",
    "\n",
    "*Key Features:*\n",
    "\n",
    "-\tSchema-Based: DataFrames support structured data with predefined schemas, which enables SQL-like operations and integration with Spark SQL.\n",
    "\n",
    "-\tOptimization: The Catalyst Optimizer automatically optimizes DataFrame queries, and the Tungsten engine further improves performance by managing memory more efficiently.\n",
    "\n",
    "*Performance Considerations:*\n",
    "\n",
    "DataFrames are significantly faster than RDDs because of these optimizations.\n",
    "\n",
    "**Ideal Use Case:** DataFrames are ideal for ETL processes and SQL-like operations on structured data.\n",
    "\n",
    "\n",
    "**Datasets:**\n",
    "\n",
    "*Key Features:*\n",
    "\n",
    "-\tType Safety: Datasets offer compile-time type checking, which helps catch errors early in development.\n",
    "\n",
    "-\tOptimized Execution: Datasets benefit from Spark SQL’s optimizations while providing a typed API.\n",
    "\n",
    "-\tFlexible Transformations: Datasets allow functional programming transformations (e.g., map, filter) similar to RDDs, but with the structure and optimization benefits of DataFrames.\n",
    "\n",
    "*Performance Considerations:*\n",
    "\n",
    "Datasets perform similarly to DataFrames, although heavy object manipulation can sometimes reduce optimization efficiency.\n",
    "\n",
    "**Ideal Use Case:** Datasets are recommended when type safety and schema structure are needed, especially with semi-structured data.\n",
    "\n",
    "\n",
    "***Differences:***\n",
    "\n",
    "**Schema:**\n",
    "\n",
    "*RDDs:* Do not have a schema; they are simply collections of objects.\n",
    "\n",
    "*DataFrames:* Have a schema, which organizes data into named columns (like a table in SQL).\n",
    "\n",
    "*Datasets:* Also have a schema and combine schema-based structure with type safety.\n",
    "\n",
    "**Type Safety:**\n",
    "\n",
    "*RDDs:* Type-safe, meaning types are checked at compile time.\n",
    "\n",
    "*DataFrames:* Not type-safe; types are inferred at runtime.\n",
    "\n",
    "*Datasets:* Type-safe, offering compile-time type checks like RDDs.\n",
    "\n",
    "**Optimization:**\n",
    "\n",
    "*RDDs:* No built-in optimizations; each operation is performed as-is, resulting in higher processing time.\n",
    "\n",
    "*DataFrames:* Optimized by Spark’s Catalyst Optimizer and Tungsten engine, enhancing query performance.\n",
    "\n",
    "*Datasets:* Optimized similarly to DataFrames through the Catalyst and Tungsten engines.\n",
    "\n",
    "**API Level:**\n",
    "\n",
    "*RDDs:* Low-level API, requiring explicit definitions for operations and transformations.\n",
    "\n",
    "*DataFrames:* High-level API, making data manipulation more concise and SQL-like.\n",
    "\n",
    "*Datasets:* High-level API, combining DataFrame ease with RDD-like functional programming.\n",
    "\n",
    "**Data Type Flexibility:**\n",
    "\n",
    "*RDDs:* Suitable for unstructured data or complex data types.\n",
    "\n",
    "*DataFrames:* Best for structured data, typically used with tabular data.\n",
    "\n",
    "*Datasets:* Suitable for both structured and semi-structured data, allowing more flexible data representations.\n",
    "\n",
    "**Use Cases:**\n",
    "\n",
    "*RDDs:* Ideal for low-level operations where full control is needed, often for unstructured or complex data transformations.\n",
    "\n",
    "*DataFrames:* Suited for SQL-like operations and structured data analysis.\n",
    "\n",
    "*Datasets:* Ideal when schema and type safety are required together, typically for semi-structured or structured data that benefits from functional transformations.\n",
    "\n",
    "\n",
    "***Summary of Differences***:\n",
    "\n",
    "-\tRDDs: Low-level, fault-tolerant, no schema, slower due to fewer optimizations.\n",
    "\n",
    "-\tDataFrames: High-level, schema-based, optimized by Catalyst and Tungsten engines.\n",
    "\n",
    "-\tDatasets: Combine type safety and schema-based optimization, bridging the gap between RDDs and DataFrames.\n",
    "\n",
    "\n",
    "In Spark 2.0, DataFrames and Datasets were unified under the Dataset API, simplifying the Spark API and making it easier for developers to work with a single high-level API with type safety.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b8d5d0c-4d13-4a89-9cd4-e110283a87f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "***Question (2)***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ed953b2-a64a-405a-babc-9d40ff68ed4c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "***Question (2.1):*** \n",
    "\n",
    "How many sensor pads are reported to be from Poland?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c3da5a0-d04f-4b6c-8901-95a88517a24e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Several ways to approah this question are presented:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd82bb99-1479-4d5c-be10-8c36df0f1d44",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create a view or table\n",
    "\n",
    "temp_table_name = \"iot_devices_json\"\n",
    "\n",
    "df.createOrReplaceTempView(temp_table_name)\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5f66379-6f7f-42ec-8e82-d0e0926a1721",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "/* Query the created temp table in a SQL cell */\n",
    "\n",
    "select * from `iot_devices_json` where cn = 'Poland' and device_name like 'sensor-pad%'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db9631f6-bb4a-42ca-8a3c-0d48af932331",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "permanent_table_name = \"iot_devices_json\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37fa6516-8ef5-4f57-a6f5-2601a44e32cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "I chose to show ways to answer this question via the use of filter() and where().\n",
    "\n",
    "'''\n",
    "\n",
    "from pyspark.sql.functions import col, asc, count, when\n",
    "TempFilter1 = df.filter(col(\"cn\") == \"Poland\").filter(col(\"device_name\").like(\"sensor-pad%\"))\n",
    "sensor_poland = TempFilter1.count()\n",
    "\n",
    "print(f\"Number of sensor pads from Poland: {sensor_poland}\")\n",
    "\n",
    "df.where((col('cn') == \"Poland\") & (col('device_name').like(\"sensor-pad%\"))).count()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58963b79-1528-44d1-b346-75681040ea85",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "***Question (2.2):***\n",
    "\n",
    "How many different LCDs (distinct colors) are present in the dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1641732-9074-4826-96c5-91f380620582",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.select(\"lcd\").distinct().show()\n",
    "distinct_color_count = df.select(\"lcd\").distinct().count()\n",
    "print(f\"Number of distinct LCD colors: {distinct_color_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9733f78c-fdfc-4d77-816f-20e4d7f75c71",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "***Question (2.3):***\n",
    "\n",
    "Finding 5 countries that have the largest number of MAC devices used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0da1da0e-30c5-4494-9839-a9e85a1dfd5f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mac_devices_df = df.filter(col(\"device_name\").like(\"device-mac%\"))\n",
    "\n",
    "# Group by country (\"cn\") and count the number of MAC devices\n",
    "country_mac_counts = mac_devices_df.groupBy(\"cn\").agg(count(\"device_name\").alias(\"mac_device_count\"))\n",
    "\n",
    "# Order by the count in descending order and select the top 5 countries\n",
    "top_countries = country_mac_counts.orderBy(\"mac_device_count\", ascending=False).limit(5)\n",
    "\n",
    "# Show the results\n",
    "top_countries.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7d548fd-cd83-4062-acbc-bfefd909b0ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "***Question (2.4):***\n",
    "\n",
    "Proposing and trying a statistical test or machine learning model to gain insight from this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2208cace-5338-4c54-8ccf-2d3f54413602",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Here are a few ideas for data analysis of this dataset. \n",
    "\n",
    "I chose to use the numerical columns to run basic clustering by dendrogram, correlation using the Pearson coefficient, a correlation heatmap, and a basic anomaly detection, as well as counting distinct devices by name and country. These are just ideas, data quality was not performed in depth, just general possible analysis directions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e45d906f-e3ea-4e35-81a0-f74ad2de09a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Select numerical columns\n",
    "selected_df = df.select(\"battery_level\", \"c02_level\", \"temp\")\n",
    "\n",
    "# Drop any rows with null values in these columns\n",
    "selected_df = selected_df.na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21737178-9507-4d7d-ae28-622ef38743e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "\n",
    "# Assemble features into a vector\n",
    "assembler = VectorAssembler(inputCols=[\"battery_level\", \"c02_level\", \"temp\"], outputCol=\"features\")\n",
    "assembled_df = assembler.transform(selected_df)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\", withStd=True, withMean=True)\n",
    "scaler_model = scaler.fit(assembled_df)\n",
    "scaled_df = scaler_model.transform(assembled_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb2278c2-b2a1-4b27-8fe8-00b575465d4a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Take a small sample to make the dendrogram plotting feasible\n",
    "sample_data = scaled_df.select(\"scaled_features\").limit(50).toPandas()\n",
    "sample_features = pd.DataFrame(sample_data[\"scaled_features\"].tolist())\n",
    "\n",
    "# Perform hierarchical clustering using SciPy\n",
    "Z = linkage(sample_features, method=\"ward\")\n",
    "\n",
    "# Plot the dendrogram\n",
    "plt.figure(figsize=(10, 7))\n",
    "dendrogram(Z, labels=sample_features.index.tolist())\n",
    "plt.title(\"Hierarchical Clustering Dendrogram\")\n",
    "plt.xlabel(\"Device Index\")\n",
    "plt.ylabel(\"Distance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d9816c1f-7718-4d89-9d3e-49c0005b4b02",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Calculate the Pearson correlation between temperature and CO2 level\n",
    "temperature_c02_corr = df.stat.corr(\"temp\", \"c02_level\")\n",
    "print(f\"The correlation between temperature and CO2 level is: {temperature_c02_corr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ff2a303-fda4-4fe0-876a-2b7e70fbac1f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# List of column pairs to analyze\n",
    "columns_to_compare = [(\"temp\", \"c02_level\"), (\"battery_level\", \"temp\"), (\"battery_level\", \"c02_level\")]\n",
    "\n",
    "# Calculate and display correlations for each pair\n",
    "for col1, col2 in columns_to_compare:\n",
    "    corr_value = df.stat.corr(col1, col2)\n",
    "    print(f\"The correlation between {col1} and {col2} is: {corr_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f403991-6d0a-49c0-ae2a-60d1ddab0f56",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Select numerical columns and convert to Pandas\n",
    "numerical_data = df.select(\"battery_level\", \"temp\", \"c02_level\", \"humidity\").toPandas()\n",
    "\n",
    "# Calculate correlations\n",
    "correlation_matrix = numerical_data.corr()\n",
    "\n",
    "# Plot the heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap=\"coolwarm\", vmin=-1, vmax=1)\n",
    "plt.title(\"Correlation Heatmap of IoT Features\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc36327d-ee04-4402-b5af-49f77a8b05b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import OneVsRest\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Prepare data for Isolation Forest or other anomaly detection\n",
    "assembler = VectorAssembler(inputCols=[\"battery_level\", \"c02_level\", \"temp\"], outputCol=\"features\")\n",
    "device_features = assembler.transform(df).select(\"features\")\n",
    "\n",
    "# Set up an Isolation Forest or use KMeans as a proxy for anomaly detection\n",
    "isolation_forest = KMeans(k=2, seed=1, featuresCol=\"features\")\n",
    "model = isolation_forest.fit(device_features)\n",
    "\n",
    "# Use the model to predict clusters\n",
    "predictions = model.transform(device_features)\n",
    "predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8679ca1-9e07-412a-8c9b-7ebd9f6fc034",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when, col, count\n",
    "\n",
    "# Step 1: Create a new DataFrame with a brand column\n",
    "device_dist = df.withColumn(\n",
    "    \"brand\",\n",
    "    when(col(\"device_name\").like(\"device-mac%\"), \"Mac\")\n",
    "    .when(col(\"device_name\").like(\"device-samsung%\"), \"Samsung\")\n",
    "    .when(col(\"device_name\").like(\"device-apple%\"), \"Apple\")\n",
    "    .when(col(\"device_name\").like(\"device-dell%\"), \"Dell\")\n",
    "    .otherwise(\"Other\")\n",
    ")\n",
    "\n",
    "# Step 2: Group by country and brand, then count devices\n",
    "device_dist = device_dist.groupBy(\"cn\", \"brand\").agg(count(\"device_name\").alias(\"device_count\"))\n",
    "\n",
    "# Show the new DataFrame with device counts by country and brand\n",
    "device_dist.show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 2042230837843414,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "2024-11-02 - Assignment_3-Ilana Berger Fridman",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
